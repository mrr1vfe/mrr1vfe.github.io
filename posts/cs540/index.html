<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Introduction to Artifical Intelligence | mrr1vfe</title><meta name=keywords content="Python"><meta name=description content="Basic Python, comparing to Java 1    Why are we using python?   It has a large amount of better machine learning libraries.
  Useful link   Online Python compiler
  Key differences from Java	   Do not bother with a class unless you actually want to make an object.
  Functions do not need return types (or parameter types)."><meta name=author content="Reid Chen"><link rel=canonical href=https://www.mrr1vfe.io/posts/cs540/><link crossorigin=anonymous href=/assets/css/stylesheet.min.35cd0f65a15cafa92372b8313deef5960aae04b90ad722f2bbf509eb0468137e.css integrity="sha256-Nc0PZaFcr6kjcrgxPe71lgquBLkK1yLyu/UJ6wRoE34=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://www.mrr1vfe.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://www.mrr1vfe.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://www.mrr1vfe.io/favicon-32x32.png><link rel=apple-touch-icon href=https://www.mrr1vfe.io/apple-touch-icon.png><link rel=mask-icon href=https://www.mrr1vfe.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.84.0"><meta property="og:title" content="Introduction to Artifical Intelligence"><meta property="og:description" content="Basic Python, comparing to Java 1    Why are we using python?   It has a large amount of better machine learning libraries.
  Useful link   Online Python compiler
  Key differences from Java	   Do not bother with a class unless you actually want to make an object.
  Functions do not need return types (or parameter types)."><meta property="og:type" content="article"><meta property="og:url" content="https://www.mrr1vfe.io/posts/cs540/"><meta property="og:image" content="https://www.mrr1vfe.io/papermod-cover.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-04-30T00:00:00+00:00"><meta property="article:modified_time" content="2020-04-30T00:00:00+00:00"><meta property="og:see_also" content="https://www.mrr1vfe.io/posts/cs532/"><meta property="og:see_also" content="https://www.mrr1vfe.io/posts/cs537/"><meta property="og:see_also" content="https://www.mrr1vfe.io/posts/cs538/"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://www.mrr1vfe.io/papermod-cover.png"><meta name=twitter:title content="Introduction to Artifical Intelligence"><meta name=twitter:description content="Basic Python, comparing to Java 1    Why are we using python?   It has a large amount of better machine learning libraries.
  Useful link   Online Python compiler
  Key differences from Java	   Do not bother with a class unless you actually want to make an object.
  Functions do not need return types (or parameter types)."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://www.mrr1vfe.io/posts/"},{"@type":"ListItem","position":2,"name":"Introduction to Artifical Intelligence","item":"https://www.mrr1vfe.io/posts/cs540/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Introduction to Artifical Intelligence","name":"Introduction to Artifical Intelligence","description":"Basic Python, comparing to Java 1    Why are we using python?   It has a large amount of better machine learning libraries.\n  Useful link   Online Python compiler\n  Key differences from Java\t   Do not bother with a class unless you actually want to make an object.\n  Functions do not need return types (or parameter types).","keywords":["Python"],"articleBody":" Basic Python, comparing to Java 1    Why are we using python?   It has a large amount of better machine learning libraries.\n  Useful link   Online Python compiler\n  Key differences from Java\t   Do not bother with a class unless you actually want to make an object.\n  Functions do not need return types (or parameter types).\n  Indentations matter, not {}; begin functions with : and end by unindenting.\n  Strings can be inside \"\" or ''; comments begin with #, and no semicolons are needed.\n  Control flow  Conditions and loops have the same indentation rules as functions. For loops are actually for each loops. Therefore some iterables objects are needed to iterate over (list, string, etc).\n  Operators    There is not ++ operator. Use +=1 instead.\n  is operator is Java's ==.\n  == operator is Java's .equals().\n    Reading files  with open('[filename]', '[mode]') as f: # closes automatically when you unindent. for line in f: print(line)    import  import is used to get access to any codes beyond the basic.\nimport math # to get access to some math functions        Think AI at different levels    AI in movies\n  AI in reality\n  AI in \"theory\"\n  AI implementation\n  Big idea in AI: State space  The search problem is to find a solution path from a state in $I$ to a state in $G$. Optionally minimize the cost of the solution\n  State space   $S$, all valid configurations\n  Initial states   $I \\in S$\n  Goal states   $G \\in S$\n  Successor function   succs($s$) $\\in S$, states reachable in one step from $s$.\n  Cost   cost(path) = $\\sum_{(\\textrm{edge} \\in \\textrm{path})} \\textrm{Cost(edge)}$\n      Hill Climbing  Optimization problems    Each state $s$ has a score $f(s)$ that we can compute.\n  The goal is to find the state with the highest score, or a reasonably high score.\n  Do not care about path.\n  Why?    Hard to know the goal state\n  Hard to know successor state\n  Hard to enumerate\n      Idea  Starting from some state $s$, and move to a neighbor $t$ with better score. Repeat this process.\n  Neighbor   You have to define it, also known as move set. It is similar to successor function.\n  Neighborhood   A set of neighbors of a given state. A neighborhood bust be small enough for efficiency.\n    Neighbor picking  If no neighbor is better than the current state, i.e. $f(s)$ is worse, then do nothing. Otherwise pick the best one (greedy).\n  Algorithm  s = initial_state() # pick the initial state s while True: neighbors = get_neighbors(s) # generate all neighbors best = best_neighbor(neighbor) # pick the neighbor with the best f score if f_score(best)  f_score(s): return s s = best  This is very greedy. Easily stuck.\n  Local optima in hill climbing  We want global optimum. There can many local optima, which we do not want.\n$s$ is local minimum if $\\forall t \\in \\textrm{succ}(s), f(s)  Repeated hill climbing with random restarts  When stuck, pick a random new start. run basic hill climbing from there. Repeat this process for $k$ times. Then return the best of the $k$ local optima. This can be very effective, and should be tried whenever hill climbing is used.\n    Basic Probability and Statistics  Great idea: Uncertainty modeled by probability. Probability is the language of uncertainty. It is the central pillar of modern day artificial intelligence.\nSample Space    A space of events that we assign probabilities to.\n  Events can be binary, multi-values, or continuous.\n  Events are mutually exclusive.\n  Examples\n  Coin flip: {head, tail}\n  Die roll: {1, 2, 3, 4, 5, 6}\n  English words: a dictionary\n      Random Variable  A variable, $x$, whose domain is the sample space, and whose value is somewhat uncertain. Examples:\n  x = coin flip outcome\n  x = first word in tomorrow's headline news\n  x = tomorrow's temperature\n    Axioms of probability    $P(A) \\in [0, 1]$\n  $P(\\textrm{True}) = 1$, $P(\\textrm{False}) = 0$.\n  $P(A \\cup B) = P(A) + P(B) - P(A \\cap B)$\n    Coin  A coin has 2 sides, head and tail. The probability of getting a head, when flipping a coin, is denoted as $P(H)$, and the probability of getting a tail is $P(T)$. And $P(H) + P(T) = 1$ since getting a head and getting a tail are the only two options, assuming the coin cannot stand. If the a coin is fair, then $P(H) = P(T) = \\frac{1}{2}$. Otherwise, $P(H) = p \\in [0, 1], P(T) = 1 - p$.\n  Law of large numbers  Flip a coin N times, let the outcomes be $x_1 \\in \\{H, T\\}, x_2 \\in \\{H, T\\}, ..., x_N \\in \\{H, T\\}$. There is a indicator function called $\\mathbb{I}$: \\begin{align*} \\mathbb{I}[Z] = \\begin{cases} 1,\u0026 \\text{if } Z \\text{ is True}\\\\ 0,\u0026 \\text{if } Z \\text{ is False} \\end{cases} \\text{ Where } Z \\text{ is a boolean function} \\end{align*} Then, \\begin{align*} limN → ∞ \\frac{∑^Ni=1\\mathbb{I}[x_i = H]}{N} = p \\end{align*} $p$ is the frequency interpretation of probability.\n  Die  Given a fair die that has 6 faces, the probability of getting each face after a roll is the same, $\\frac{1}{6}$. However, if a die is loaded, or unfair, then the probability of getting each face, $P_1 ... P_6$ is $\\sum^6_{j=1}P_j = 1, P_j \\in [0, 1], j=1...6$. The outcome of a die rolling is $x \\in \\{1, 2, 3, 4, 5, 6\\}$. Then, \\begin{align*} limN → ∞ \\frac{∑^Ni=1\\mathbb{I}[x_i = j]}{N} = p_j \\text{, for } j = 1 … 6 \\end{align*}\n  Joint probability  $P(A, B)$ → Both events $A$ and $B$ are true.\n  Negation (complement)  $\\bar{A} = \\neg A = A^c$ $P(\\bar{A}) = 1 - P(A)$\n  Marginalization  $P(A, B) + P(\\bar{A}, B) = P(B)$\n  Conditional Probability  $P(A|B)$ is the probability of A given B (is observed). \\begin{align*} P(A | B) = \\frac{P(A, B)}{P(B)} = \\frac{P(A, B)}{P(A, B) + P(\\bar{A} + B)} \\end{align*}\nBayes Rule  \\begin{align*} P(F|H) \u0026= \\frac{P(F, H)}{P(H)} P(H|F) \u0026= \\frac{P(H, F)}{P(F)} = \\frac{P(F, H)}{P(F)}\nP(F, H) \u0026= P(H|F)P(F) \\frac{P(F, H)}{P(H)} \u0026= \\frac{P(H|F)P(F)}{P(H)} P(F|H) \u0026= \\frac{P(F, H)}{P(H)} = \\frac{P(H|F)P(F)}{P(H)} \\end{align*}\n    Independence  Two events A, B are independent if:\n  P(A, B) = P(A) * P(B)\n  P(A | B) = P(A)\n  P(B | A) = P(B)\n    Conditional Independence  Random variables can be dependent, but conditionally independent. In general, A, B are conditionally independent given C if\n  P(A|B, C) = P(A | C) or\n  P(B|A, C) = P(B | C) or\n  P(A, B|C) = P(A|C) * P(B|C)\n      Tuning set  To minimize over-fitting, we can use a tuning set.\n  We get a labeled data set $(x_1, y_1) \\cdot \\cdot \\cdot (x_N, y_N)$.\n  Randomly split the data set into 3 sets.\n  First, shuffle those N data items\n  Take some fraction (e.g. 60%) of the shuffled item, and call them training set\n  Take another fraction (e.g. 20%), and call them tuning set\n  The remaining items are test set\n  You want the training set to be large enough, but you also want the tuning set and the test set to be not so small\n    Train $\\hat{\\beta}^{(0)}, \\hat{\\beta}^{(1)}, \\cdot\\cdot\\cdot \\hat{\\beta}^{(n-1)}$ on training set.   Measure their tuning set MSE.\n  Pick the best model, using\n    \\begin{align*} \\hat{j}^* = argminj = 0…n-1 [\\textrm{tuning-set MSE}(\\hat{β}(j))] \\end{align*} where \\begin{align*} \\textrm{tuning-set MSE}(\\hat{β}(j)) = \\textrm{average of }L(x, y, \\hat{β}(j))\\textrm{on tuning points } (x, y) \\end{align*}\n  We pick model $\\hat{\\beta}^{\\hat{j}^*}$\n    We report test-set MSE with model $\\hat{\\beta}^{\\hat{j}^*}$.\n    K nearest neighbor classifier  Recall    Unsupervised Learning, Data: $x_1 \\cdot\\cdot\\cdot x_n$\n  Dimension Reduction\n  Clustering\n      HAC\n  kmeans\n    Supervised, Training Data $(x_i, y_i), i \\in [1, n]$\n  Regression, $y \\in R$\n  Classification, $y$ discrete finite \"classes\"\n      Naive Bayes\n  KNN\n    KNN algorithm    input 1   $(x_1, y_1) \\cdot\\cdot\\cdot (x_n, y_n),\\textrm{ where } x_i \\in R^d, y_i \\textrm{ is a class label}$\n  input 2   A distance function, $dist(x, x')$. e.g. Euclidean distance $|x - x'|$\n    Given a new item $x \\in R^d$, find the K nearest neighbors of x in the training set under $dist()$.\n  Predict a label $\\hat{y}$ as the majority label of the K nearest neighbor. (Break tie arbitrarily).\n    Classification vs. Clustering    Terminology    0-1 Loss function   $L(x, y, \\hat{y}) = \\begin{cases} 1, \u0026 \\text{if}\\ y \\neq \\hat{y} \\text{ mis-prediction} \\\\ 0, \u0026 \\text{if } y = \\hat{y}\\end{cases} = \\textrm{Indicator}[y_i \\neq \\hat{y_i}]$, where $\\hat{y}$ is the predicted label\n  Training Set Error (rate)    Training set = $(x_1, y_1) \\cdot\\cdot\\cdot (x_n, y_n)$ \\begin{align*} \\frac{1}{n}∑ni=1L(x_i, y_i, \\hat{y_i}) = \\frac{1}{n}∑ni=1\\textrm{Indicator}[y_i ≠q \\hat{y_i}] \\end{align*}\n  Test Set Error    Test set = $(x_{n + 1}, y_{n + 1}) \\cdot\\cdot\\cdot (x_{n + m}, y_{n + m})$ \\begin{align*} \\frac{1}{n}∑n + mi=n+1\\textrm{Indicator}[y_i ≠q \\hat{y_i}] \\end{align*}\n  Why do we need a test set?    Machine learning assumes an underlying joint distribution \\begin{align*} p(x, y) \\text{ , unknown but fixed} \\end{align*} Training set is an independent and identically-distributed (i.i.d) sample from $p$. \\begin{align*} (x_i, y_i) \\textrm{} p(x, y) \\\\ (x_n, y_n) \\textrm{} p(x, y) \\end{align*} Test set is also an iid sample from $p$. Test set and Training set have the same underlying distribution. The future item that will be applied to the model also has the same underlying distribution.\n  True error    \\begin{align*} \\textrm{EXP}_{(x, y) \\textrm{~} p(x, y)} \\textrm{Indicator}[y_i ≠q \\hat{y_i}] \\text{ not computable} \\end{align*} Since we cannot compute this true error, we use test set to evaluate the model.\n  Accuracy    \\begin{align*} \\textrm{Accuracy} = 1 - \\textrm{error} \\end{align*}\n    How to choose k?  Method 1: Use a tuning set    randomly shuffle\n  split into training, tuning, and test set.\n  $\\hat{k} = \\textrm{argmin}_{k = 1, 2, ...}$ [tuning error with respect to kNN predictions (from training set)]\n  report $\\hat{k}NN$ prediction (from training set)'s test error.\n  If you use training error on kNN, then the training error is going to favor $k = 1$.\n  Method 2: Cross Validation    Start with full dataset, $(x_1, y_1) \\cdot\\cdot\\cdot (x_N, y_N) \\textrm{~} p$, split to two sets. The second set is test set\n  K-fold Cross Validation\n  Evenly split the first set into $k$ folds, $\\textrm{fold }1 \\cdot\\cdot\\cdot \\textrm{fold }k$\n  For $i = 1 \\cdot\\cdot\\cdot k$\t      use fold $i$ as the tuning set\n  and folds $1\\cdot\\cdot\\cdot k$ excepts $i$ as the training set\n  get tuning error $E_i$\n  Pick the model parameter ($k$ in $kNN$)\n    \\begin{align*} \\hat{k}kNN = \\textrm{argmin}_{kkNN}\\frac{1}{k_{\\textrm{fold}}}∑^{kfold}i = 1E_i \\end{align*}\n  In practice, $k \\in [5, 10]$\n  Retrain model on all folds as training set\n  Report test set error\n  Train $k+1$ times total\n        Logistic Regression  Recall    Supervised, Training Data $(x_i, y_i), i \\in [1, n]$\n  Regression, $y \\in R$\n      Linear Regression\n  $y = x^Tw + \\epsilon, y \\in R, x \\in R^(d+1), x = \\begin{bmatrix} 1\\\\.\\\\.\\\\.\\\\x_d\\end{bmatrix}, w = \\begin{bmatrix} w_0\\\\.\\\\.\\\\.\\\\w_d\\end{bmatrix}$\n  Classification, $y$ discrete finite \"classes\"\n      Naive Bayes\n  KNN\n  Logistic Regression\n    Logistic Regression    Input (Training data)    $(x_1, y_1) \\cdot\\cdot\\cdot (x_n, y_n)$ where \\begin{align*} \u0026x_i = \\begin{bmatrix} 1\\\\x_{i1}\\\\.\\\\.\\\\.\\\\x_{id}\\end{bmatrix} ∈ Rd+1 \u0026y_i ∈ \\{-1, 1\\} \\text{ binary classification or}, \u0026y_i ∈ \\{1, 2, 3, ⋅⋅⋅, k\\} k\\text{-classes} \\end{align*}\n  Let's start with binary classification\n  Binary Classification  Try to estimate conditional probability \\begin{align*} P_w(y=1|x) = \\frac{1}{1 + exp(-x^Tw)} \\text{ Sigmoid function} \\end{align*} if $x^T = 0, \\frac{1}{1 + e^{-0}} = \\frac{1}{1 + 1} = \\frac{1}{2}$ if $x^T = \\infty, \\frac{1}{1 + e^{-\\infty}} = \\frac{1}{1 + 0} = 1$ if $x^T = -\\infty, \\frac{1}{1 + e^{\\infty}} = \\frac{1}{1 + \\infty} = 0$ $x^Tw$ represents how strongly is the label going to be 1. \\begin{align*} P_w(y=1|x) = 1 - P_w(y=1, x) \\end{align*} For $y \\in \\{-1, 1\\}$, binary classification $P_w(y|x) = \\frac{1}{1+e^{-yx^Tw}}$\n  Training    Training   Estimate $s \\in R^{d+1}$ from training data (more later)\n  Prediction   Given a new item $x \\in R^{d+1}$, predict its label\n  \\begin{align*} \\hat{y} = \\textrm{argmax}_yP_w(y|x) \\end{align*}\n  K-class Logistic Regression  $y \\in {1, 2, 3, \\cdot\\cdot\\cdot, k}$ $w^{(1)} = \\begin{bmatrix} w^{(1)}_0\\\\.\\\\.\\\\.\\\\w^{(1)}_d\\end{bmatrix}$ $w^{(2)} = \\begin{bmatrix} w^{(2)}_0\\\\.\\\\.\\\\.\\\\w^{(2)}_d\\end{bmatrix}$ … $w^{(k-1)} = \\begin{bmatrix} w^{(k-1)}_0\\\\.\\\\.\\\\.\\\\w^{(k-1)}_d\\end{bmatrix}$ $w^{(k)} = \\begin{bmatrix} 0\\\\.\\\\.\\\\.\\\\0\\end{bmatrix}$ $P_w(y|x) = \\frac{e^{x^Tw^{(j)}}}{\\sum^{K}_{k=1}e^{x^Tw^{(k)}}}$, where $w$ is a collection of $w^{(1)}, \\cdot\\cdot\\cdot, w^{(k)}$\n  Example      1 The whole note is based on and is coming from the course materials of COMP SCI 540 by Professor Jerry Zhu and Hobbes LeGault.\n    ","wordCount":"1928","inLanguage":"en","datePublished":"2020-04-30T00:00:00Z","dateModified":"2020-04-30T00:00:00Z","author":{"@type":"Person","name":"Reid Chen"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://www.mrr1vfe.io/posts/cs540/"},"publisher":{"@type":"Organization","name":"mrr1vfe","logo":{"@type":"ImageObject","url":"https://www.mrr1vfe.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script><noscript><style type=text/css>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:#1d1e20;--entry:#2e2e33;--primary:rgba(255, 255, 255, 0.84);--secondary:rgba(255, 255, 255, 0.56);--tertiary:rgba(255, 255, 255, 0.16);--content:rgba(255, 255, 255, 0.74);--hljs-bg:#2e2e33;--code-bg:#37383e;--border:#333}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><header class=header><nav class=nav><div class=logo><a href=https://www.mrr1vfe.io/ accesskey=h title="mrr1vfe (Alt + H)">mrr1vfe</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://www.mrr1vfe.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://www.mrr1vfe.io/categories title=Categories><span>Categories</span></a></li><li><a href=https://www.mrr1vfe.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://www.mrr1vfe.io/series title=Series><span>Series</span></a></li><li><a href=https://www.mrr1vfe.io/tags title=Tags><span>Tags</span></a></li><li><a href=https://archive.casouri.cat/ title=BHL0388><span>BHL0388</span></a></li><li><a href=https://archive.casouri.cat/goldfish/index.html title=金鱼><span>金鱼</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Introduction to Artifical Intelligence</h1><div class=post-meta>April 30, 2020&nbsp;·&nbsp;10 min&nbsp;·&nbsp;Reid Chen</div></header><div class=post-content><div id=outline-container-headline-1 class=outline-2><h2 id=headline-1>Basic Python, comparing to Java <sup class=footnote-reference><a id=footnote-reference-1 href=#footnote-1>1</a></sup></h2><div id=outline-text-headline-1 class=outline-text-2><dl><dt>Why are we using python?</dt><dd><p>It has a large amount of better machine learning libraries.</p></dd><dt>Useful link</dt><dd><p><a href=https://repl.it/languages/python3>Online Python compiler</a></p></dd></dl><div id=outline-container-headline-2 class=outline-3><h3 id=headline-2>Key differences from Java</h3><div id=outline-text-headline-2 class=outline-text-3><ul><li><p>Do not bother with a class unless you actually want to make an object.</p></li><li><p>Functions do not need return types (or parameter types).</p></li><li><p>Indentations matter, not <code class=verbatim>{}</code>; begin functions with <code class=verbatim>:</code> and end by unindenting.</p></li><li><p>Strings can be inside <code class=verbatim>""</code> or <code class=verbatim>''</code>; comments begin with <code class=verbatim>#</code>, and no semicolons are needed.</p></li></ul><div id=outline-container-headline-3 class=outline-4><h4 id=headline-3>Control flow</h4><div id=outline-text-headline-3 class=outline-text-4><p>Conditions and loops have the same indentation rules as functions.
For loops are actually for each loops. Therefore some iterables objects are needed to iterate over (list, string, etc).</p></div></div><div id=outline-container-headline-4 class=outline-4><h4 id=headline-4>Operators</h4><div id=outline-text-headline-4 class=outline-text-4><ul><li><p>There is not <code class=verbatim>++</code> operator. Use <code class=verbatim>+=1</code> instead.</p></li><li><p><code class=verbatim>is</code> operator is Java's <code class=verbatim>==</code>.</p></li><li><p><code class=verbatim>==</code> operator is Java's <code class=verbatim>.equals()</code>.</p></li></ul></div></div><div id=outline-container-headline-5 class=outline-4><h4 id=headline-5>Reading files</h4><div id=outline-text-headline-5 class=outline-text-4><div class="src src-python"><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>with</span> open(<span style=color:#e6db74>&#39;[filename]&#39;</span>, <span style=color:#e6db74>&#39;[mode]&#39;</span>) <span style=color:#66d9ef>as</span> f: <span style=color:#75715e># closes automatically when you unindent.</span>
    <span style=color:#66d9ef>for</span> line <span style=color:#f92672>in</span> f:
 print(line)</code></pre></div></div></div></div><div id=outline-container-headline-6 class=outline-4><h4 id=headline-6>import</h4><div id=outline-text-headline-6 class=outline-text-4><p><code class=verbatim>import</code> is used to get access to any codes beyond the basic.</p><div class="src src-python"><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python> <span style=color:#f92672>import</span> math <span style=color:#75715e># to get access to some math functions</span></code></pre></div></div></div></div></div></div></div></div><div id=outline-container-headline-7 class=outline-2><h2 id=headline-7>Think AI at different levels</h2><div id=outline-text-headline-7 class=outline-text-2><ol><li><p>AI in movies</p></li><li><p>AI in reality</p></li><li><p>AI in "theory"</p></li><li><p>AI implementation</p></li></ol><div id=outline-container-headline-8 class=outline-3><h3 id=headline-8>Big idea in AI: State space</h3><div id=outline-text-headline-8 class=outline-text-3><p>The search problem is to find a solution path from a state in $I$ to a state in $G$. Optionally minimize the cost of the solution</p><dl><dt>State space</dt><dd><p>$S$, all valid configurations</p></dd><dt>Initial states</dt><dd><p>$I \in S$</p></dd><dt>Goal states</dt><dd><p>$G \in S$</p></dd><dt>Successor function</dt><dd><p>succs($s$) $\in S$, states reachable in one step from $s$.</p></dd><dt>Cost</dt><dd><p>cost(path) = $\sum_{(\textrm{edge} \in \textrm{path})} \textrm{Cost(edge)}$</p></dd></dl></div></div></div></div><div id=outline-container-headline-9 class=outline-2><h2 id=headline-9>Hill Climbing</h2><div id=outline-text-headline-9 class=outline-text-2><div id=outline-container-headline-10 class=outline-3><h3 id=headline-10>Optimization problems</h3><div id=outline-text-headline-10 class=outline-text-3><ul><li><p>Each state $s$ has a <code class=verbatim>score</code> $f(s)$ that we can compute.</p></li><li><p>The goal is to find the state with the <code class=verbatim>highest score</code>, or a reasonably high score.</p></li><li><p>Do not care about path.</p></li></ul><div id=outline-container-headline-11 class=outline-4><h4 id=headline-11>Why?</h4><div id=outline-text-headline-11 class=outline-text-4><ul><li><p>Hard to know the goal state</p></li><li><p>Hard to know successor state</p></li><li><p>Hard to enumerate</p></li></ul></div></div></div></div><div id=outline-container-headline-12 class=outline-3><h3 id=headline-12>Idea</h3><div id=outline-text-headline-12 class=outline-text-3><p>Starting from some state $s$, and move to a neighbor $t$ with better score. Repeat this process.</p><dl><dt>Neighbor</dt><dd><p>You have to define it, also known as <code class=verbatim>move set</code>. It is similar to successor function.</p></dd><dt>Neighborhood</dt><dd><p>A set of neighbors of a given state. A neighborhood bust be small enough for efficiency.</p></dd></dl></div></div><div id=outline-container-headline-13 class=outline-3><h3 id=headline-13>Neighbor picking</h3><div id=outline-text-headline-13 class=outline-text-3><p>If no neighbor is better than the current state, i.e. $f(s)$ is worse, then do nothing. Otherwise pick the best one (greedy).</p></div></div><div id=outline-container-headline-14 class=outline-3><h3 id=headline-14>Algorithm</h3><div id=outline-text-headline-14 class=outline-text-3><div class="src src-python"><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>  s <span style=color:#f92672>=</span> initial_state() <span style=color:#75715e># pick the initial state s</span>
  <span style=color:#66d9ef>while</span> <span style=color:#66d9ef>True</span>:
neighbors <span style=color:#f92672>=</span> get_neighbors(s) <span style=color:#75715e># generate all neighbors</span>
best <span style=color:#f92672>=</span> best_neighbor(neighbor) <span style=color:#75715e># pick the neighbor with the best f score</span>
<span style=color:#66d9ef>if</span> f_score(best) <span style=color:#f92672>&lt;=</span> f_score(s):
   <span style=color:#66d9ef>return</span> s
s <span style=color:#f92672>=</span> best</code></pre></div></div><p>This is very greedy. Easily stuck.</p></div></div><div id=outline-container-headline-15 class=outline-3><h3 id=headline-15>Local optima in hill climbing</h3><div id=outline-text-headline-15 class=outline-text-3><p>We want global optimum. There can many local optima, which we do not want.<br>$s$ is local minimum if $\forall t \in \textrm{succ}(s), f(s) &lt; f(t)$.<br>$s$ is global minimum if $\forall t \in S, f(s) &lt; f(t)$</p></div></div><div id=outline-container-headline-16 class=outline-3><h3 id=headline-16>Repeated hill climbing with random restarts</h3><div id=outline-text-headline-16 class=outline-text-3><p>When stuck, pick a random new start. run basic hill climbing from there. Repeat this process for $k$ times. Then return the best of the $k$ local optima. This can be very effective, and should be tried whenever hill climbing is used.</p></div></div></div></div><div id=outline-container-headline-17 class=outline-2><h2 id=headline-17>Basic Probability and Statistics</h2><div id=outline-text-headline-17 class=outline-text-2><p>Great idea: Uncertainty modeled by probability.
Probability is the language of uncertainty. It is the central pillar of modern day artificial intelligence.</p><div id=outline-container-headline-18 class=outline-3><h3 id=headline-18>Sample Space</h3><div id=outline-text-headline-18 class=outline-text-3><ul><li><p>A space of events that we assign probabilities to.</p></li><li><p>Events can be binary, multi-values, or continuous.</p></li><li><p>Events are mutually exclusive.</p></li><li><p>Examples</p><ul><li><p>Coin flip: {head, tail}</p></li><li><p>Die roll: {1, 2, 3, 4, 5, 6}</p></li><li><p>English words: a dictionary</p></li></ul></li></ul></div></div><div id=outline-container-headline-19 class=outline-3><h3 id=headline-19>Random Variable</h3><div id=outline-text-headline-19 class=outline-text-3><p>A variable, $x$, whose domain is the sample space, and whose value is somewhat uncertain.
Examples:</p><ul><li><p>x = coin flip outcome</p></li><li><p>x = first word in tomorrow's headline news</p></li><li><p>x = tomorrow's temperature</p></li></ul></div></div><div id=outline-container-headline-20 class=outline-3><h3 id=headline-20>Axioms of probability</h3><div id=outline-text-headline-20 class=outline-text-3><ul><li><p>$P(A) \in [0, 1]$</p></li><li><p>$P(\textrm{True}) = 1$, $P(\textrm{False}) = 0$.</p></li><li><p>$P(A \cup B) = P(A) + P(B) - P(A \cap B)$</p></li></ul></div></div><div id=outline-container-headline-21 class=outline-3><h3 id=headline-21>Coin</h3><div id=outline-text-headline-21 class=outline-text-3><p>A coin has 2 sides, head and tail. The probability of getting a head, when flipping a coin, is denoted as $P(H)$, and the probability of getting a tail is $P(T)$. And $P(H) + P(T) = 1$ since getting a head and getting a tail are the only two options, assuming the coin cannot stand. If the a coin is fair, then $P(H) = P(T) = \frac{1}{2}$. Otherwise, $P(H) = p \in [0, 1], P(T) = 1 - p$.</p></div></div><div id=outline-container-headline-22 class=outline-3><h3 id=headline-22>Law of large numbers</h3><div id=outline-text-headline-22 class=outline-text-3><p>Flip a coin N times, let the outcomes be $x_1 \in \{H, T\}, x_2 \in \{H, T\}, ..., x_N \in \{H, T\}$. There is a indicator function called $\mathbb{I}$:
\begin{align*}
\mathbb{I}[Z] =
\begin{cases}
1,& \text{if } Z \text{ is True}\\
0,& \text{if } Z \text{ is False}
\end{cases}
\text{ Where } Z \text{ is a boolean function}
\end{align*}
Then,
\begin{align*}
lim<sub>N → ∞</sub> \frac{∑^N<sub>i=1</sub>\mathbb{I}[x_i = H]}{N} = p
\end{align*}
$p$ is the frequency interpretation of probability.</p></div></div><div id=outline-container-headline-23 class=outline-3><h3 id=headline-23>Die</h3><div id=outline-text-headline-23 class=outline-text-3><p>Given a fair die that has 6 faces, the probability of getting each face after a roll is the same, $\frac{1}{6}$. However, if a die is loaded, or unfair, then the probability of getting each face, $P_1 ... P_6$ is $\sum^6_{j=1}P_j = 1, P_j \in [0, 1], j=1...6$. The outcome of a die rolling is $x \in \{1, 2, 3, 4, 5, 6\}$. Then,
\begin{align*}
lim<sub>N → ∞</sub> \frac{∑^N<sub>i=1</sub>\mathbb{I}[x_i = j]}{N} = p_j \text{, for } j = 1 … 6
\end{align*}</p></div></div><div id=outline-container-headline-24 class=outline-3><h3 id=headline-24>Joint probability</h3><div id=outline-text-headline-24 class=outline-text-3><p>$P(A, B)$ → Both events $A$ and $B$ are true.</p></div></div><div id=outline-container-headline-25 class=outline-3><h3 id=headline-25>Negation (complement)</h3><div id=outline-text-headline-25 class=outline-text-3><p>$\bar{A} = \neg A = A^c$<br>$P(\bar{A}) = 1 - P(A)$</p></div></div><div id=outline-container-headline-26 class=outline-3><h3 id=headline-26>Marginalization</h3><div id=outline-text-headline-26 class=outline-text-3><p>$P(A, B) + P(\bar{A}, B) = P(B)$</p></div></div><div id=outline-container-headline-27 class=outline-3><h3 id=headline-27>Conditional Probability</h3><div id=outline-text-headline-27 class=outline-text-3><p>$P(A|B)$ is the probability of A given B (is observed).
\begin{align*}
P(A | B) = \frac{P(A, B)}{P(B)} = \frac{P(A, B)}{P(A, B) + P(\bar{A} + B)}
\end{align*}</p><div id=outline-container-headline-28 class=outline-4><h4 id=headline-28>Bayes Rule</h4><div id=outline-text-headline-28 class=outline-text-4><p>\begin{align*}
P(F|H) &= \frac{P(F, H)}{P(H)}<br>P(H|F) &= \frac{P(H, F)}{P(F)} = \frac{P(F, H)}{P(F)}<br>P(F, H) &= P(H|F)P(F)<br>\frac{P(F, H)}{P(H)} &= \frac{P(H|F)P(F)}{P(H)}<br>P(F|H) &= \frac{P(F, H)}{P(H)} = \frac{P(H|F)P(F)}{P(H)}
\end{align*}</p></div></div></div></div><div id=outline-container-headline-29 class=outline-3><h3 id=headline-29>Independence</h3><div id=outline-text-headline-29 class=outline-text-3><p>Two events A, B are <code class=verbatim>independent</code> if:</p><ul><li><p>P(A, B) = P(A) * P(B)</p></li><li><p>P(A | B) = P(A)</p></li><li><p>P(B | A) = P(B)</p></li></ul></div></div><div id=outline-container-headline-30 class=outline-3><h3 id=headline-30>Conditional Independence</h3><div id=outline-text-headline-30 class=outline-text-3><p>Random variables can be dependent, but conditionally independent.
In general, A, B are conditionally independent given C if</p><ul><li><p>P(A|B, C) = P(A | C) or</p></li><li><p>P(B|A, C) = P(B | C) or</p></li><li><p>P(A, B|C) = P(A|C) * P(B|C)</p></li></ul></div></div></div></div><div id=outline-container-headline-31 class=outline-2><h2 id=headline-31>Tuning set</h2><div id=outline-text-headline-31 class=outline-text-2><p>To minimize over-fitting, we can use a <span style=text-decoration:underline>tuning set</span>.</p><ol><li><p>We get a labeled data set $(x_1, y_1) \cdot \cdot \cdot (x_N, y_N)$.</p></li><li><p>Randomly split the data set into 3 sets.</p><ol><li><p>First, shuffle those N data items</p></li><li><p>Take some fraction (e.g. 60%) of the shuffled item, and call them <code class=verbatim>training set</code></p></li><li><p>Take another fraction (e.g. 20%), and call them <code class=verbatim>tuning set</code></p></li><li><p>The remaining items are <code class=verbatim>test set</code></p></li><li><p>You want the <code class=verbatim>training set</code> to be large enough, but you also want the <code class=verbatim>tuning set</code> and the <code class=verbatim>test set</code> to be not so small</p></li></ol></li><li><p>Train $\hat{\beta}^{(0)}, \hat{\beta}^{(1)}, \cdot\cdot\cdot \hat{\beta}^{(n-1)}$ on <code class=verbatim>training set</code>.</p></li><li><p>Measure their <code class=verbatim>tuning set</code> MSE.</p><ol><li><p>Pick the best model, using</p></li></ol></li></ol><p>\begin{align*}
\hat{j}^* = argmin<sub>j = 0…n-1</sub> [\textrm{tuning-set MSE}(\hat{β}<sup>(j)</sup>)]
\end{align*}
where
\begin{align*}
\textrm{tuning-set MSE}(\hat{β}<sup>(j)</sup>) = \textrm{average of }L(x, y, \hat{β}<sup>(j)</sup>)\textrm{on tuning points } (x, y)
\end{align*}</p><ol><li><p>We pick model $\hat{\beta}^{\hat{j}^*}$</p></li></ol><ol><li><p>We report <code class=verbatim>test-set</code> MSE with model $\hat{\beta}^{\hat{j}^*}$.</p></li></ol></div></div><div id=outline-container-headline-32 class=outline-2><h2 id=headline-32>K nearest neighbor classifier</h2><div id=outline-text-headline-32 class=outline-text-2><div id=outline-container-headline-33 class=outline-3><h3 id=headline-33>Recall</h3><div id=outline-text-headline-33 class=outline-text-3><ol><li><p>Unsupervised Learning, Data: $x_1 \cdot\cdot\cdot x_n$</p><ol><li><p>Dimension Reduction</p></li><li><p>Clustering</p></li></ol></li></ol><ol><li><p>HAC</p></li><li><p>kmeans</p></li></ol><ol><li><p>Supervised, Training Data $(x_i, y_i), i \in [1, n]$</p><ol><li><p>Regression, $y \in R$</p></li><li><p>Classification, $y$ discrete finite "classes"</p></li></ol></li></ol><ol><li><p>Naive Bayes</p></li><li><p>KNN</p></li></ol></div></div><div id=outline-container-headline-34 class=outline-3><h3 id=headline-34>KNN algorithm</h3><div id=outline-text-headline-34 class=outline-text-3><dl><dt>input 1</dt><dd><p>$(x_1, y_1) \cdot\cdot\cdot (x_n, y_n),\textrm{ where } x_i \in R^d, y_i \textrm{ is a class label}$</p></dd><dt>input 2</dt><dd><p>A distance function, $dist(x, x')$. e.g. Euclidean distance $|x - x'|$</p></dd></dl><ol><li><p>Given a new item $x \in R^d$, find the K nearest neighbors of x in the training set under $dist()$.</p></li><li><p>Predict a label $\hat{y}$ as the majority label of the K nearest neighbor. (Break tie arbitrarily).</p></li></ol></div></div><div id=outline-container-headline-35 class=outline-3><h3 id=headline-35>Classification vs. Clustering</h3><div id=outline-text-headline-35 class=outline-text-3><p><img src=./classification.jpeg alt=./classification.jpeg title=./classification.jpeg></p><p><img src=./clustering.jpeg alt=./clustering.jpeg title=./clustering.jpeg></p></div></div><div id=outline-container-headline-36 class=outline-3><h3 id=headline-36>Terminology</h3><div id=outline-text-headline-36 class=outline-text-3><dl><dt>0-1 Loss function</dt><dd><p>$L(x, y, \hat{y}) = \begin{cases} 1, & \text{if}\ y \neq \hat{y} \text{ mis-prediction} \\ 0, & \text{if } y = \hat{y}\end{cases} = \textrm{Indicator}[y_i \neq \hat{y_i}]$, where $\hat{y}$ is the predicted label</p></dd><dt>Training Set Error (rate)</dt><dd><p>Training set = $(x_1, y_1) \cdot\cdot\cdot (x_n, y_n)$
\begin{align*}
\frac{1}{n}∑<sup>n</sup><sub>i=1</sub>L(x_i, y_i, \hat{y_i}) = \frac{1}{n}∑<sup>n</sup><sub>i=1</sub>\textrm{Indicator}[y_i ≠q \hat{y_i}]
\end{align*}</p></dd><dt>Test Set Error</dt><dd><p>Test set = $(x_{n + 1}, y_{n + 1}) \cdot\cdot\cdot (x_{n + m}, y_{n + m})$
\begin{align*}
\frac{1}{n}∑<sup>n + m</sup><sub>i=n+1</sub>\textrm{Indicator}[y_i ≠q \hat{y_i}]
\end{align*}</p></dd><dt>Why do we need a test set?</dt><dd><p>Machine learning assumes an underlying joint distribution
\begin{align*}
p(x, y) \text{ , unknown but fixed}
\end{align*}
Training set is an independent and identically-distributed (i.i.d) sample from $p$.
\begin{align*}
(x_i, y_i) \textrm{<code>} p(x, y) \\
(x_n, y_n) \textrm{</code>} p(x, y)
\end{align*}
Test set is also an iid sample from $p$. Test set and Training set have the same underlying distribution. The future item that will be applied to the model also has the same underlying distribution.</p></dd><dt>True error</dt><dd><p>\begin{align*}
\textrm{EXP}_{(x, y) \textrm{~} p(x, y)} \textrm{Indicator}[y_i ≠q \hat{y_i}] \text{ not computable}
\end{align*}
Since we cannot compute this true error, we use test set to evaluate the model.</p></dd><dt>Accuracy</dt><dd><p>\begin{align*}
\textrm{Accuracy} = 1 - \textrm{error}
\end{align*}</p></dd></dl></div></div><div id=outline-container-headline-37 class=outline-3><h3 id=headline-37>How to choose k?</h3><div id=outline-text-headline-37 class=outline-text-3><div id=outline-container-headline-38 class=outline-4><h4 id=headline-38>Method 1: Use a tuning set</h4><div id=outline-text-headline-38 class=outline-text-4><ol><li><p>randomly shuffle</p></li><li><p>split into training, tuning, and test set.</p></li><li><p>$\hat{k} = \textrm{argmin}_{k = 1, 2, ...}$ [tuning error with respect to kNN predictions (from training set)]</p></li><li><p>report $\hat{k}NN$ prediction (from training set)'s test error.</p></li></ol><p>If you use training error on kNN, then the training error is going to favor $k = 1$.</p></div></div><div id=outline-container-headline-39 class=outline-4><h4 id=headline-39>Method 2: Cross Validation</h4><div id=outline-text-headline-39 class=outline-text-4><ul><li><p>Start with full dataset, $(x_1, y_1) \cdot\cdot\cdot (x_N, y_N) \textrm{~} p$, split to two sets. The second set is test set</p></li><li><p>K-fold Cross Validation</p><ul><li><p>Evenly split the first set into $k$ folds, $\textrm{fold }1 \cdot\cdot\cdot \textrm{fold }k$</p></li><li><p>For $i = 1 \cdot\cdot\cdot k$</p></li></ul></li></ul><ul><li><p>use fold $i$ as the tuning set</p></li><li><p>and folds $1\cdot\cdot\cdot k$ excepts $i$ as the training set</p></li><li><p>get tuning error $E_i$</p><ul><li><p>Pick the model parameter ($k$ in $kNN$)</p></li></ul></li></ul><p>\begin{align*}
\hat{k}<sub>kNN</sub> = \textrm{argmin}_{k<sub>kNN</sub>}\frac{1}{k_{\textrm{fold}}}∑^{k<sub>fold</sub>}<sub>i = 1</sub>E_i
\end{align*}</p><ul><li><p>In practice, $k \in [5, 10]$</p></li><li><p>Retrain model on all folds as training set</p></li><li><p>Report test set error</p></li><li><p>Train $k+1$ times total</p></li></ul></div></div></div></div></div></div><div id=outline-container-headline-40 class=outline-2><h2 id=headline-40>Logistic Regression</h2><div id=outline-text-headline-40 class=outline-text-2><div id=outline-container-headline-41 class=outline-3><h3 id=headline-41>Recall</h3><div id=outline-text-headline-41 class=outline-text-3><ol><li><p>Supervised, Training Data $(x_i, y_i), i \in [1, n]$</p><ol><li><p>Regression, $y \in R$</p></li></ol></li></ol><ol><li><p>Linear Regression</p><ul><li><p>$y = x^Tw + \epsilon, y \in R, x \in R^(d+1), x = \begin{bmatrix} 1\\.\\.\\.\\x_d\end{bmatrix}, w = \begin{bmatrix} w_0\\.\\.\\.\\w_d\end{bmatrix}$</p><ol><li><p>Classification, $y$ discrete finite "classes"</p></li></ol></li></ul></li><li><p>Naive Bayes</p></li><li><p>KNN</p></li><li><p>Logistic Regression</p></li></ol></div></div><div id=outline-container-headline-42 class=outline-3><h3 id=headline-42>Logistic Regression</h3><div id=outline-text-headline-42 class=outline-text-3><dl><dt>Input (Training data)</dt><dd><p>$(x_1, y_1) \cdot\cdot\cdot (x_n, y_n)$ where
\begin{align*}
&x_i = \begin{bmatrix} 1\\x_{i1}\\.\\.\\.\\x_{id}\end{bmatrix} ∈ R<sup>d+1</sup><br>&y_i ∈ \{-1, 1\} \text{ binary classification or},<br>&y_i ∈ \{1, 2, 3, ⋅⋅⋅, k\} k\text{-classes}
\end{align*}</p></dd></dl><p>Let's start with binary classification</p></div></div><div id=outline-container-headline-43 class=outline-3><h3 id=headline-43>Binary Classification</h3><div id=outline-text-headline-43 class=outline-text-3><p>Try to estimate conditional probability
\begin{align*}
P_w(y=1|x) = \frac{1}{1 + exp(-x^Tw)} \text{ Sigmoid function}
\end{align*}
if $x^T = 0, \frac{1}{1 + e^{-0}} = \frac{1}{1 + 1} = \frac{1}{2}$<br>if $x^T = \infty, \frac{1}{1 + e^{-\infty}} = \frac{1}{1 + 0} = 1$<br>if $x^T = -\infty, \frac{1}{1 + e^{\infty}} = \frac{1}{1 + \infty} = 0$<br>$x^Tw$ represents how strongly is the label going to be 1.
\begin{align*}
P_w(y=1|x) = 1 - P_w(y=1, x)
\end{align*}
For $y \in \{-1, 1\}$, binary classification $P_w(y|x) = \frac{1}{1+e^{-yx^Tw}}$</p></div></div><div id=outline-container-headline-44 class=outline-3><h3 id=headline-44>Training</h3><div id=outline-text-headline-44 class=outline-text-3><dl><dt>Training</dt><dd><p>Estimate $s \in R^{d+1}$ from training data (more later)</p></dd><dt>Prediction</dt><dd><p>Given a new item $x \in R^{d+1}$, predict its label</p></dd></dl><p>\begin{align*}
\hat{y} = \textrm{argmax}_yP_w(y|x)
\end{align*}</p></div></div><div id=outline-container-headline-45 class=outline-3><h3 id=headline-45>K-class Logistic Regression</h3><div id=outline-text-headline-45 class=outline-text-3><p>$y \in {1, 2, 3, \cdot\cdot\cdot, k}$
$w^{(1)} = \begin{bmatrix} w^{(1)}_0\\.\\.\\.\\w^{(1)}_d\end{bmatrix}$<br>$w^{(2)} = \begin{bmatrix} w^{(2)}_0\\.\\.\\.\\w^{(2)}_d\end{bmatrix}$<br>…<br>$w^{(k-1)} = \begin{bmatrix} w^{(k-1)}_0\\.\\.\\.\\w^{(k-1)}_d\end{bmatrix}$<br>$w^{(k)} = \begin{bmatrix} 0\\.\\.\\.\\0\end{bmatrix}$<br>$P_w(y|x) = \frac{e^{x^Tw^{(j)}}}{\sum^{K}_{k=1}e^{x^Tw^{(k)}}}$, where $w$ is a collection of $w^{(1)}, \cdot\cdot\cdot, w^{(k)}$</p></div></div><div id=outline-container-headline-46 class=outline-3><h3 id=headline-46>Example</h3><div id=outline-text-headline-46 class=outline-text-3><p><img src=./k_class_example.png alt=./k_class_example.png title=./k_class_example.png></p></div></div></div></div><div class=footnotes><hr class=footnotes-separatator><div class=footnote-definitions><div class=footnote-definition><sup id=footnote-1><a href=#footnote-reference-1>1</a></sup><div class=footnote-body><p>The whole note is based on and is coming from the course materials of <a href=http://pages.cs.wisc.edu/~jerryzhu/cs540.html>COMP SCI 540</a> by Professor Jerry Zhu and Hobbes LeGault.</p></div></div></div></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://www.mrr1vfe.io/tags/python/>Python</a></li></ul><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Introduction to Artifical Intelligence on twitter" href="https://twitter.com/intent/tweet/?text=Introduction%20to%20Artifical%20Intelligence&url=https%3a%2f%2fwww.mrr1vfe.io%2fposts%2fcs540%2f&hashtags=Python"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Introduction to Artifical Intelligence on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fwww.mrr1vfe.io%2fposts%2fcs540%2f&title=Introduction%20to%20Artifical%20Intelligence&summary=Introduction%20to%20Artifical%20Intelligence&source=https%3a%2f%2fwww.mrr1vfe.io%2fposts%2fcs540%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Introduction to Artifical Intelligence on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fwww.mrr1vfe.io%2fposts%2fcs540%2f&title=Introduction%20to%20Artifical%20Intelligence"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Introduction to Artifical Intelligence on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fwww.mrr1vfe.io%2fposts%2fcs540%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Introduction to Artifical Intelligence on whatsapp" href="https://api.whatsapp.com/send?text=Introduction%20to%20Artifical%20Intelligence%20-%20https%3a%2f%2fwww.mrr1vfe.io%2fposts%2fcs540%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Introduction to Artifical Intelligence on telegram" href="https://telegram.me/share/url?text=Introduction%20to%20Artifical%20Intelligence&url=https%3a%2f%2fwww.mrr1vfe.io%2fposts%2fcs540%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2021 <a href=https://www.mrr1vfe.io/>mrr1vfe</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)"><button class=top-link id=top-link type=button accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></button></a>
<script>let menu=document.getElementById('menu');menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)},document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script></body></html>